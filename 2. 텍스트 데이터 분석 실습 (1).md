# 2. 텍스트 데이터 분석 실습 (1)
## (1) 텍스트 분석의 종류
  - 문서 분류
  - 감정 분석
  - 정보 추출

<br>
<hr>
<br>

## (2) 문서 분류(Text Classification)란?
  - 문서를 카테고리, 주제, 장르 등의 기준으로 분류하는 작업
  - 독립 변수 : 문서(document)인 d
  
    종속 변수 : 클래스(장르)인 C = {c1, c2, ..., cn)
  - 스팸 필터링, 언어 확인, 감성 분석(Sentiment Analysis)


<br>
<hr>
<br>


## (3) 문서 분류 : 규칙(Rule-Based)에 따른 문서 분류
  - 비교적 단순한 규칙에 의해서 문서를 분류하는 방식이다.
  - 사람이 규칙을 일일이 정해주기 때문에 규칙을 만들고 유지 관리하는 비용이 크다. -> 기계학습 알고리즘을 사용하는게 유용함
  - 스팸 필터링 : 'dollars'와 'loan'이 같이 등장할 경우 스팸 메일로 처리


<br>
<hr>
<br>


## (4) 문서 분류 : 지도학습 알고리즘을 사용한 문서 분류
###  1) Input
   - 독립 변수 : 문서(document)인 d
   
     종속 변수 : 클래스(장르)인 C = {c1, c2, ..., cn)
     
     training set : 이미 분류된 문서와 그 문서가 속한 클래스의 쌍인 (d1, c1), (d2, c3), (d3, c3), (d4, cn), ..., (dm, c1)

###  2) Output
   - 학습된 문서 분류기(classifier) : 문서를 받으면 클래스를 분류한다. (d->c)


<br>
<hr>
<br>


## (5) 문서 분류 : Naive Bayes를 사용한 문서 분류
###  1) Bayes' rule
   - Bayes' rule : 문서 d가 클래스 c에 속할 확률
   
     P(c|d) = (P(d|c)P(c)) / P(d)

###  2) 부류가 여러 개일 경우
   - 각각의 부류에 해당하는 조건부 확률 계산
   - 입력 문서 : 고정되어 있음
   - 가장 높은 확률을 가지는 클래스 = 해당 문서가 속한 클래스


<br>
<hr>
<br>


## (6) 문서 분류 : Naive Bayes를 사용한 문서 분류 식
  - 하나의 클래스에 해당 문서가 속할 확률 계산
  - 목적 : 문서 d가 속할 확률이 가장 높은 클래스 c를 찾아내는 것
###  1) 식 구하기
   - c = argmax P(c|d)
       = argmax (P(d|c)P(c)) / P(d)
       = argmax P(d|c)P(c)
       = argmax P(x1, x2, ..., xn|c)P(c)

###  2) 최종 식
   - x1, x2, x3 ..., xn은 문서 d의 속성이다.
   
     문서 d의 각 속성 값이 클래스 c에 속할 확률을 구한다.
     
     P(x1, x2, ..., xn|c) = P(x1|c)*P(x2|c)* ... *P(xn|c)
     
   - c = argmax∏(x∈X)P(x|c)P(c)

###  3) 빈도수 계산
   - P'(c) = doccount(C=c) / Ndoc
   
     P'(x|c) = count(x,c) / ∑(x∈V)count(x,c)


<br>
<hr>
<br>


## (7) 문서 분류 : Naive Bayes를 사용한 문서 분류 예시
  - 문서 분류의 예제로 5개의 데이터 셋이 있다. 독립 변수인 Words들에 따라 Class가 분류된 데이터이다.
###  1) Words에 따른 Action, SF 구분 데이터

| Doc No. |	Words |				Class |
|---|:---:|---:|
| 1 |	Fast, fly, furious, furious	| Action |
| 2 |	Future, technology, space	| SF |
| 3 |	Fly, drive, future, fast, fast |	Action | 
| 4 |	Technology, space, space, fast |	SF |
| 5 |	Drive, future, space, furious |	SF |

###  2) 테스트 문서 d에 future, technology, fast가 등장할 때 어느 클래스에 해당할까?
   - 각 클래스에 단어가 등장하는 빈도수 계산
   
     count(futre, action) = 1 #문서에 future가 등장했을 때 action 클래스였던 적 1회
     
     count(technology, action) = 0
     
     count(fast, action) = 3
     
     count(future, sf) = 2
     
     count(technology, sf) = 2
     
     count(fast, sf) = 1

###  3) 빈도수를 사용해 확률 계산 : P(action|d)
   - 방법
   
     d에 등장하는 단어마다 각 장르에서 등장할 확률을 구한 다음, 그 확률들을 모두 곱하고 해당 클래스가 전체에서 등장할 확률을 곱한다.
     
     d에 등장하는 단어는 future, technology, fast이다.
     
     action 장르에 속한 문서들에 있는 모든 단어의 개수는 9개인데, future는 한 번 등장한다. P(future|action) = 1/9
     
     action은 전체 5개 문서 중 2개의 문서를 가진다. 2/5
     
   - 식
   
     P(action|d) = P(future|action) * P(technology|action) * P(fast|action)
     
		= 1/9 * 0/9 * 3/9 * 2/5 = 0

###  4) 빈도수를 사용해 확률 계산 : P(sf|d)
   - 방법
   
     sf 장르에 속한 문서들에 있는 모든 단어의 개수는 11개인데, future는 두 번 등장한다. P(future|sf) = 2/11
     
     sf는 전체 5개 문서 중 3개의 문서를 가진다. 3/5
     
   - 식
   
     P(sf|d) = P(future|sf) * P(technology|sf) * P(fast|sf)
     
	   = 2/11 * 2/11 * 1/11 * 3/5 = 0.0018

###  5) 빈도수를 사용해 확률 계산 : 계산 결과
   - 앞의 3번과 4번의 계산 결과 SF에 속할 확률이 더 높으므로 문서 d를 SF로 분류한다!
   - 단점 : 이러한 계산 방법은 학습 데이터에 없는 새로운 단어가 있는 문서를 분류하려 하면 모든 계산식이 0이 된다.
   - Laplace Smoothing 기법을 통해서 문제를 해결할 수 있다!

###  6) Laplace Smoothing이란?
   - 새로운 단어가 등장하더라도 해당 빈도에 1을 더해 확률이 0이 되는 것을 방지한다.
   - 확률을 곱할 때 하나라도 0이 되면 다른 것과 상관 없이 전체가 0이 되기 때문에 이를 방지한다.
   
   - P'(x|c) = (count(x,c) + 1) / (∑x∈V(count(x,c) + 1))
   
	   = (count(x,c) + 1) / ((∑x∈V(count(x,c)) + |V|)

###  7) Laplace Smoothing을 사용해 확률 계산
   - Laplace Smoothing을 사용해 확률 계산
   
     앞에서 구한 것을 Laplace Smoothing을 사용해 다시 계산해 본다. 결과는 이전과 같이 SF로 분류된다.
     
     하지만 Laplace Smoothing의 효과로 action에 속할 확률이 0이 되는 문제를 해결했다.
     
   - P(action|d)
   
     = P(future|action) * P(technology|action) * P(fast|action) * P(aciton)
     
     = ((1+1) / (9+7)) * ((0+1) / (9+1)) * ((3+1) / (9+1)) * (2/5) = 0.00078
     
   - P(sf|d) 
   
     = P(future|sf) * P(technology|sf) * P(fast|sf) * P(sf)
     
     = ((2+1) / (11+7)) * ((2+1) / (11+7)) * ((1+1) / (11+7)) * (3/5) = 0.0018


<br>
<hr>
<br>


## (8) 감성 분석(Sentiment Analysis) 이란?
  - 텍스트로부터 태도, 의견, 성향 등의 정보를 추출하는 방법
  - 의견 속의 긍정적, 중립적, 부정적인 감성을 문장을 통해서 파악
  - 행복, 분노, 슬픔 등의 감정을 세분화 해 분석 가능
  - 맛집의 댓글에 남겨진 좋았다, 맛있었다, 맛없었다, 불친절했다 등의 내용을 모아 감성 분석을 해서 음식점을 평가할 수 있다.
  - 영화 리뷰 글이 긍정적인지 부정적인지 분석
  - 상품 후기를 통한 고객들의 반응 분석을 통해 시장 움직임을 예측할 수 있다.
  - 시민들의 정책에 대한 감정 분석을 통해 투표 결과를 예측할 수 있다.


<br>
<hr>
<br>


## (9) 감성 분석 : 감성 분석의 3가지 심화 단계
  1) 긍정, 부정 구분
  2) 감성의 강도를 1~5의 수치로 추출
  3) 특정 감정을 추출, 감정 발생 원인과 대상 추출


<br>
<hr>
<br>


## (10) 감성 분석 : Naive Bayes를 사용한 감성 분석
###  1) 토큰화 진행
###  2) 토큰들의 속성 추출
   - 모든 토큰들 중 감성을 나타내는 형용사만 추출한다. (depress, angry, sad, warm)
###  3) 문서의 감정 분류
   - 문서 분류와 같은 방법이다.
   - 단, 문서 분류와의 차이점은 감성 분류는 클래스가 긍정, 중성, 부정으로 구분된다는 점이다.


<br>
<hr>
<br>


## (11) 감성 분석 : 영문 텍스트 분석 실습 과정 1
###  1) 영어 뉴스 텍스트 데이터 수집
###  2) 데이터 전처리(Pre-Processing)
   - 토큰화 : 텍스트를 의미 단위로 분할하는 작업
   - 정제 : 무의미한 토큰을 제거하는 작업
   - 형태소 분석 : 토큰의 형태소를 파악하는 작업
###  3) Word Cloud 형태로 시각화
###  4) 특징값/속성 추출(Feature Selection)
   - 의미 단위가 가지는 수치정보, 가중치
###  5) 분류 모델 속성
   - 기계학습 알고리즘을 사용한 텍스트 분류 모델 학습
###  6) 분류 결과
   - 뉴스 분류. 분류 결과 추출 및 분류기 성능 평가


<br>
<hr>
<br>


## (12) 감성 분석 : 영문 텍스트 분석 실습 과정 2
###  1) 뉴스 데이터 수집
   - 마이크로소프트사에서 제공하는 Bing 뉴스 검색 API를 사용해 데이터를 수집하기

###  2) API Key 발급
   - API : 각 기관이나 개인이 제공하는 데이터를 사용하는 방법. API를 사용하기 위해서는 API Key를 발급받아야 된다.
   - https://azure.microsoft.com/ko-kr/ 접속 -> 상단 메뉴 '제품' 선택 -> Bing News Search 검색 -> 아래의 '설명서' 선택 -> 왼쪽 메뉴의 Resources -> Get API Key 선택

###  3) 소스코드 작성 : 뉴스 수집을 위한 모듈 및 패키지 import
   - http.client : 데이터를 요청하는 client의 http 프로토콜을 관리하는 패키지
   - urllib.request : URL을 통해 데이터를 읽어들이는 패키지
   - urllib.parse : 읽어온 데이터를 문법적으로 분석하는 모듈
   - urllib.error : request에서 발생하는 오류 처리
   - base64 : 읽어온 이진 형태의 데이터를 ASCII 형태로 변환
   - json : json 스트링이나 파일을 파싱하는 패키지
   
   ```python
  import http.client, urllib.request, urllib.parse, urllib.error, base64
  import json
  import pandas as pd
  import numpy as np
  ```

###  4) 소스코드 작성 : 수집한 뉴스 데이터를 저장할 데이터 프레임 생성
   - Name : 뉴스 제목
   - Description : 뉴스 요약 내용
   - Category : 뉴스 카테고리
   
   ```python
  df = pd.DataFrame(columns = ('name', 'description', 'category')) #컬럼 명이 name, description, category인 데이터 프레임 구성
  ```

###  5) 소스코드 작성 : 발급받은 API 키 입력
   - dictionary 타입의 변수에 발급받은 API key를 입력한다.
   - 이 코드를 통해 실제로 실습하고 싶을 때는, 'Ocp-Apim-subscription-key':는 그대로 쓰고 그 밑에 발급 받은 key를 입력한다.
   
   ```python
  headers = {
	'Ocp-Apim-subscription-key':
		'47e483110279412994d41873c430600c',
	#발급 키는 17년 6월까지 유효
  }
  ```

###  6) 소스코드 작성 : World 카테고리의 뉴스 수집을 위한 변수 설정
   - Bing News 전체를 받아오는 것이 아니고, 특정 시장의 특정 카테고리의 데이터를 수집할 지 정하여 받아오는 것이다. (카테고리, 마켓, 개수 지정)
   - 이번에는 World 카테고리의 뉴스 수집을 위해 변수를 설정한다. 이후의 과정에서 카테고리를 다른 것으로 변경하여 이 과정을 반복하여 여러 카테고리의 데이터를 모을 것이다.
   
   ```python
  params = urlib.parse.urlencode({
	#Business, Entertainment, health, politics, ScienceandTechnology, Sports, UK, World
	'Category':'World', #World 카테고리
	'Market':'en-GB', #en-GB 마켓
	'Count':100
  })
  ```

###  7) 소스코드 작성 : 뉴스 데이터 요청
   - try문에서 일련의 작업을 시도하고, 실패할 경우에는 except 문을 사용해서 예외처리를 한다.
   - try문 안에서 지정된 URL과 연결한다. 연결 요청 정보를 URL 뒤 변수로 선언하고 request 함수를 사용해서 데이터를 요청한다.
   - 뉴스 데이터를 받아서 변수에 저장하고 출력해 본다.
   
   ```python
  try:
	conn = http.client.HTTPSConnection('api.cognitive.microsoft.com) #http 프로토콜을 사용해 데이터를 요청하는 http.client의 모듈 중 HTTPSConnection을 사용해 지정된 URL과 연결한다.
	conn.request("GET", "/bing/v7.0/news/?%s" %params, "{body}", headers) #request 함수를 사용해서 데이터를 요청한다.
	response = conn.getresponse()
	data = response.read() #뉴스 데이터를 받아서 data 변수에 저장한다.
	print(data) #변수 data에 저장된 byte 형태의 정보 출력
	conn.close()
  except Exception as e:
	print("[Errno{0}]{1}".format(e.errno, e.strerror))
  ```

###  8) 소스코드 작성 : data에 저장된 byte 형태 정보를 보기 편하게 가공

```python
  data = data.decode('utf-8') #data에 저장된 뉴스데이터를 utf-8 형태로 변환한다.
  obj = json.loads(data) #data에 저장된 뉴스데이터를 Json 형태로 또 변환한다.
  val = obj['value'] #Obj 중 뉴스 데이터의 내용만 포함하고 있는 'value' 값을 val 변수에 저장한다.
  val[0].keys() #val의 key 값만 출력한다.
  #dict_keys(['url', 'category', 'clusteredArticles', 'description', 'datePublished', 'image', 'about', 'name', rovideri)
  ```

###  9) 소스코드 작성 : 가공된 뉴스 데이터를 데이터프레임에 입력
   - 처음에 만들어 둔 데이터 프레임에 뉴스 데이터를 입력하기.
   
   ```python
  l = len(df)
  for i in range(0, len(val)) :
	df.loc[l+i] = [val[i]['name'], val[1]['description'], #val에 저장된 데이터 중 name, description, category 내용만 저장
		       val[i]['category']]
  df
#	name		description		category
0	Concerns ove..	LONDON Concern that..	Business
1	Global stock..	SEOUL. South Korea..	Business
```

###  10) 소스코드 작성 : 다양한 카테고리의 기사를 수집하기 위해 앞의 작업 반복
   - 앞에서 데이터를 요청하기 위해 입력했던 params의 category 값을 변경해서 
   
     같은 데이터 프레임에 계속 입력해 주는 작업을 반복하여 다양한 카테고리의 기사들을 수집한다.
     
```python
  params = urlib.parse.urlencode({
	#Business, Entertainment, health, politics, ScienceandTechnology, Sports, UK, World
	'Category':'World', #이 카테고리 부분을 다른 값으로 바꿔서 다양한 카테고리의 기사를 수집한다!
	'Market':'en-GB', #en-GB 마켓
	'Count':100
  })
```

###  11) 소스코드 작성 : 셔플
   - 카테고리들에 대한 수집을 완료한 후 카테고리가 골고루 분포하도록 랜덤으로 섞어 저장한다.
   
```python
  df_sh = df.iloc[np.random.permutation(len(df))] #np.random.permutation 함수를 사용해 0부터 (df의 길이 -1)까지의 숫자를 random으로 반환하여 셔플한다.
						  #iloc 함수를 사용해 랜덤으로 주어진 인덱스에 해당하는 데이터프레임 df의 데이터를 새로운 데이터 프레임 df_sh에 저장.
  df_sh.to_csv('bing_news_shuffle_0517.csv') #랜덤으로 섞인 뉴스 데이터가 저장된 df_sh를 csv 파일로 저장한다.
```
