# 3. 텍스트 데이터 분석 실습 (2)
## (1) 데이터 전처리란?
  - 기계학습 알고리즘에 데이터를 효과적으로 적용하기 위한 작업
  - 데이터의 특정 속성이 없거나 정보가 누락되어 있는 불완전한 데이터를 필터링
  - 데이터에 포함된 잘못된 값이나 이상값과 같은 노이즈 제거
  - 데이터에 충돌하는 값, 불일치하는 값을 제거

<br>
<hr>
<br>

## (2) 텍스트 데이터 전처리
  - 토큰화(Tokenize) : 텍스트를 단어나 구와 같은 의미 단위로 분할하는 작업
  - 정제(Cleaning) : 텍스트에 포함된 무의미한 단어, 불용어를 제거하는 작업
  - 형태소 분석(POS-Tagging) : 토큰의 품사를 파악하는 작업


<br>
<hr>
<br>


## (3) 정제
###  1) 정제란?
   - 텍스트에 포함된 무의미한 단어, 불용어를 제거하는 작업이다.
   - 불용어 : 관사 a, an, the 지시대명사 this, that 등
   - 불용어는 아예 고려 대상에서 제거한 뒤 해당 문장이나 문서로부터 특징을 추출하는게 좋다.
   - The shopping center stays open until 9 a.m. -> The, until 불용어

###  2) stopwords
   - NLTK 패키지에서는 샘플 corpus(말뭉치, 문서 집합)를 제공한다.
   - corpus는 단순한 소설, 뉴스 등의 문서 외에도 쉬운 분석을 위해 제공된다.
   - corpus의 stopwords 예제는 불용어를 담고 있다. 정제할 텍스트 데이터에서 stopwords를 제거하면 정제 가능하다!
   
     Stopwords : 덴마크어, 네덜란드어, 핀란드어, 프랑스어 등 다양한 언어의 불용어 제공


<br>
<hr>
<br>


## (4) 토큰화 : Bi-gram
  - Bi-gram은 단어들을 두 개씩 묶어서 하나의 토큰으로 추출한 것이다.
  - The shopping center stays open until 9 a.m.
  
      -> the, shopping
      
      -> shopping, center
      
      -> center, stays
      
      -> stays, open
      
      -> open, until
      
      -> until, 9
      
      -> 9, a.m.


<br>
<hr>
<br>


## (5) 토큰화 : Tri-gram
  - Tri-gram은 단어들을 세 개씩 묶어서 하나의 토큰으로 추출한 것이다.
  - The shopping center stays open until 9 a.m.
  
      -> the, shopping, center
      
      -> shopping, center, stays
            
      -> center, stays, open
      
      -> stays, open, until
      
      -> open, until, 9
      
      -> until, 9, a.m.


<br>
<hr>
<br>


## (6) 시각화 : Word Cloud
###  1) Word Cloud란?
   - 문서에 등장한 단어의 빈도를 측정해 시각화하는 방법이다.
   - 등장 빈도가 높은 단어를 크게 표현하고, 등장 빈도가 낮은 단어를 작게 표현한다.
   - 문서의 중심 내용을 한 눈에 파악할 수 있다.

###  2) 패키지 다운로드
   - Anaconda Prompt에서 python -V(혹은 python --version)로 파이썬의 버전을 확인한다.
   - http://www.lfd.uci.edu/~gohlke/pythonlibs/#wordcloud 에 들어가서 파이썬 버전, OS 비트에 맞는 설치 파일을 받는다.
   
     wordcloud-1.3.1-cp36-cpp36m-win.amd64.whl
   - Anaconda 설치 폴더(C:\Users\사용자이름\Anaconda3) 아래에 받은 파일 넣기.
   - Anaconda Prompt에서 (C:\Users\사용자이름\Anaconda3) 위치로 이동한 후 pip install wordcloud~.whl 입력하기.


<br>
<hr>
<br>


## (7) 문서 단어 행렬
  - 각 문서와 단어에 해당하는 수치를 표현한 행렬.
  - TF * IDF 값으로 문서 단어 행렬을 구성할 수 있다.


<br>
<hr>
<br>


## (8) 영문 뉴스 데이터 분석 실습
###  1) 토큰화 : 라이브러리
   - 영문 자연어 처리 패키지 NLTK(Natural Language ToolKit) 라이브러리를 이용한다.
   
     nltk 라이브러리 설치 : Anaconda Prompt -> 주피터 노트북 실행 폴더로 가기 -> activate tensorflow_env -> pip install nltk
   - nltk.download()를 통해 NLTK의 Tokenization 모듈을 다운로드 받아야 한다. 창이 뜨면 상단의 Models - Identifier가 punkt인 항목 선택 - 다운로드 한다.
   - 영문 뉴스 내용에 따른 category를 학습하여, 예측해볼 수 있도록 한다.
   
   ```python
  import pandas as pd
  import numpy as np
  import nltk
  nltk.download() #Tokenization 모듈 다운
  df = pd.DataFrame(columns=('name', 'description', 'category')) #df에 name, description, category 열 생성
  df = pd.read_csv('bing_news_shuffle.csv', encoding = 'cp949') #미리 준비 된 뉴스 데이터 파일을 읽어온다.
  df
  print(df.ix[0]['description']) #첫번째 행의 뉴스 내용 출력
  ```

###  2) 토큰화 : 공백 기반
   - NLTK의 word_tokenize 함수를 사용해 위 뉴스 내용을 단순 공백 기반으로 토큰화 해 출력해 본다.
   
   ```python
  tokens = nltk.word_tokenize(df.ix[0]['description']) #첫번째 행의 뉴스 내용을 nltk.word_tokenize로 공백 기반 토큰화한다!
  tokens
#['As', 'different', 'the', 'type', 'general', 'of', 'election', 'campaign', 'ploughs', 'is', 'on', ',', 'a' ...]
```

###  3) 토큰화 : 선택 저장
   - 길이가 1보다 큰 문자들을 소문자로 변경하여 저장한다.
   
   ```python
  tokens = [token.lower() for token in tokens if len(token) > 1]
  tokens
#['as', 'campaign', 'the', 'is', 'general', 'raging', 'election', 'in', 'ploughs', 'on', 'different', 'type', 'of' ...]
```

###  4) 토큰화 : Bi-gram
   - 앞에서 추출한 tokens에 NLTK의 bigrams 함수를 적용해 bigram을 추출하고 tokens_bigram에 저장
   
   ```python
  tokens_bigram = nltk.bigrams(tokens)
  for token in tokens_bigram :
	print(token)
  ```

###  5) 토큰화 : Tri-gram
   - 앞에서 추출한 tokens에 NLTK의 trigrams 함수를 적용해 trigram을 추출하고 tokens_trigram에 저장
   
   ```python
  tokens_trigram = nltk.trigrams(tokens)
  for token in tokens_trigram :
	print(token)
  ```

###  6) 정제 : stopwords 다운
   - Nltk에서 제공하는 불용어 리스트인 stopwords를 다운받기 위해 nltk.download()를 실행한다.
   - 창이 하나 뜨는데, corpora 중에서 stopwords를 선택 후 다운로드한다. (설치 경로는 변경하지 않는게 좋다.)
   
   ```python
  nltk.download() #stopwords를 다운 받은 후 라이브러리를 로드할 수 있다.
  from nltk.corpus import stopwords #stopwords 라이브러리 로드
  stop_words = stopwords.words('english') #영어 불용어를 가져와 stop_words에 저장한다.
  stop_words
#['i', 'myself', 'ours', 'me', 'we', 'ourselves', 'my', 'our', 'you', ...]
```

###  7) 정제 : stopwords 제거
   - tokens에 저장된 token들에 대해서, 각 token이 stopwords에 존재하는지 확인한다.
   - stopwords에 존재하지 않는 token을 token_clean에 저장해서 출력한다.
   
   ```python
  tokens_clean = [token for token in tokens if not token in stop_words]
  tokens_clean
#['general', 'buckingham', 'election', 'territory', 'ploughs', 'nestled', 'different', 'type', 'campaign', 'raging', ...]
```

###  8) 형태소 분석 : 토큰에 품사 태그하기
   - 패키지 설치 : nltk.download() -> Models 메뉴 선택 -> averaged_perceptron_tag 선택 -> Download 클릭
   - NLTK의 내장 형태소 분석기 pos_tag를 사용해 tokens_clean에 저장된 토큰들에 대해 품사를 태그해 tokens_tagged에 저장한다.
   - JJ는 형용사, NN은 명사, VBG는 동사이다.
   - 형태소 분석 결과의 tag 의미를 확인해 보려면 sltk.help.upenn_tagset()을 이용하면 된다.
   
   ```python
  nltk.download()
  tokens_tagged = nltk.pos_tag(tokens_clean)
  print(tokens_tagged)
#[('election', 'NN'), ('ploughs', 'VBZ'), ('different', 'JJ'), ('type', 'NN'), ('campaign', 'NN') ...]
```

###  9) 형태소 분석 : 명사 추출
   - 형태소 분석이 된 tokens_tagged의 토큰(wod)과 형태소(pos)에 대해 for문을 사용해 작업을 진행한다.
   - 품사가 NN(명사) 혹은 NNP(고유명사)인 경우만 tokens_noun에 저장한다.
  
  ```python
  tokens_noun = [word for word, pos in tokens_tagged if pos in ['NN', 'NNP']]
  print(tokens_noun)
#['election', 'type', 'campaign', 'constituency', 'territory', 'aylesbury', 'milton', 'parliament' ...]
```

###  10) 시각화 : Word Cloud 라이브러리 로드
   - 위에서 배운 대로 Word Cloud 패키지를 설치한 후 진행한다.
   
   ```python
  %matplotlib inline
  import matplotlib.pyplot as plt #시각화 지원
  from wordcloud import WordCloud, STOPWORDS #wordcloud, stopwords 모듈 생성
  ```

###  11) 시각화 : Word Cloud 모듈 생성

```python
  stopwords = set(STOPWORDS) #STOPWORDS를 set 함수를 사용해 변수 stopwords에 저장
  wc = WordCloud(background_color = "white", max_font_size = 100, max_words = 50, stopwords = stopwords) #WordCloud 함수를 사용해 word cloud를 그리는 모듈 wc 생성
  #배경색, 가장 큰 단어의 크기, 단어의 최대 개수, 표시 안 할 불용어 리스트를 지정해 준다.
  ```

###  12) 시각화 : 데이터 준비 및 적용

```python
  text_data = df['description'].str.cat(sep=',') #뉴스 내용인 description을 string 형태로 변환하고 cat 함수를 사용해 한 문자열로 연결한다. sep=','로 구분자를 지정해준다.
  wordcloud = wc.generate(text_data) #generate 함수를 사용해 문자열 text_data에 대해 word cloud를 그려 변수 wordcloud에 저장
  ```

###  13) 시각화 : Word Cloud 시각화

```python
  plt.figure(figsize=(10, 10)) #figsize로 공간의 크기를 설정한다.
  plt.imshow(wordcloud, iterpolation = 'bilinear') #imshow로 배열 형태의 wordcloud를 시각화 한다.
  plt.axis('off') #axis를 off로 설정하여 fig의 x축과 y축을 삭제한다. 화면에 시각화한 결과가 보이게 된다.
  ```

###  14) 단어행렬 구성(특징값 추출) : 라이브러리 로드
   - 뉴스에 어떤 단어들이 어떤 빈도로 나왔는지를 통해 특징값을 추출한다.
   - TfidVectorizer 모듈을 사용해 텍스트 데이터의 TF*IDF 값으로 문서 단어 행렬을 구성한다.
   
   ```python
  from sklearn.feature_extraction.text import TfidfVectorizer
  ```

###  15) 단어행렬 구성(특징값 추출) : 배열 변환
   - 텍스트 데이터를 astype 함수를 사용하여 문자열로 바꾼 다음, tolist 함수를 사용해 리스트로 변환한다.
   
   ```python
  text_data_list = df['description'].astype(str).tolist() #텍스트 데이터를 문자열로 바꾼 다음 다시 리스트로 변환해서 text_data_list에 저장한다.
  text_data_arr = np.array([''.join(text) for text in text_data_list]) #array 함수와 for문을 사용해서 배열로 변환한다.
  ```

###  16) 단어행렬 구성(특징값 추출) : 단어 행렬 만들기
   - TF*IDF 문서 단어 행렬을 만드는 모듈 vectorizer를 선언한다.
   - Min_df = 2 : 2번 이상 등장하는 단어
   - Ngram_range = (1,2) : 단어 추출 단위는 unigram과 bigram이다.
   - Strip_accents = 'unicode' : unicode에 해당하는 모든 문자의 accents(억양표시)를 제거한다.
   - Norm = 'L2' : pearson 함수를 사용해서 normalization을 진행한다.
   - Vectorizer의 Fit_transform 함수 : 함수를 사용해서 배열에 저장된 데이터의 문서단어행렬을 구할 수 있다.
   
   ```python
  vectorizer = TfidfVectorizer(min_df=2, ngram_range=(1,2), strip_accents='unicode', norm='l2')
  text_data = vectorizer.fit_transform(text_data_arr) #text_data에 문서 단어 행렬을 저장한다.
  ```

###  17) 단어행렬 구성(특징값 추출) : 단어 행렬 출력
   - 문서 단어 행렬을 데이터 프레임에 담아서 출력해 본다.
   - 각 행은 뉴스 문서 번호이고, 열은 단어 토큰들로 구성되어 있다. 문서 단어 행렬에, 문서에 따라 단어가 어떤 빈도인지 기록된다.
   
   ```python
  df_tfidf = pd.DataFrame(text_data.A, columns=vectorizer.get_feature_names()
  df_tfidf
  ```

###  18) 뉴스 분류 : 성능 측정 패키지 임포트

```python
  from sklearn.metrics import confusion_matrix #분류 결과 건수를 나타냄
  from sklearn.metrics import classification_report #recall, precision, f-measure
  from sklearn.metrics import f1_score #f-measure
  from sklearn.metrics import accuracy_score #정확도
```

###  19) 뉴스 분류 : 데이터 셋 준비
   - 뉴스 내용인 description과 카테고리를 각각 리스트형 변수로 변환한다.
   
   ```python
  description = df['description'].astype(str).toslist()
  category = df['description'].astype(str).tolist()
  ```

###  20) 뉴스 분류 : 데이터 셋 분할
   - 데이터 셋의 80%는 training set으로, 나머지 20%는 test set으로 구성한다.
   - x_train : 뉴스 내용 description의 training set 범위 내 내용 data를 join 함수를 사용해 연결하고 x_train에 array로 저장
   - y_train : 뉴스 카테고리 category 데이터를 y_train에 array로 저장
   
  ```python
  trainset_size = int(round(len(description)*0.80))

  x_train = np.array([''.join(data) for data in description[0:trainset_size]])
  y_train = np.array([data for data in category[0:trainset_size]])

  x_test = np.array([''.join(data) for data in description[trainset_size + 1 : len(description)]])
  y_test = np.array([data for data in category[trainset_size + 1 : len(category)]])
  ```

###  21) 뉴스 분류 : 데이터 셋을 문서 단어 행렬로 구성하기
   - 독립변수 데이터 셋을 Fit transform 함수를 사용해 문서단어행렬로 만들어 독립변수로 이용한다.
   
   ```python
  X_train = vectorizer.fit_transform(x_train)
  X_test = vectorizer.transform(x_test)
  ```

###  22) 뉴스 분류 : 각 분류 모델의 성능을 기록할 데이터 프레임 선언
   - 여러 분류 모델로 학습을 시켜볼 건데, 각 분류 모델의 성능을 기록할 데이터 프레임을 선언한다.
   
   ```python
  df_per = pd.DataFrame(colums=['Classifier', 'F-Measure', 'Accuracy'])
  df_per
  ```

###  23) 뉴스 분류 - Naive Bayes : 모델 학습

```python
  from sklearn.naive_bayes import MultinomialNB #라이브러리 로드
  nb_classifier = MultinomialNB().fit(X_train, y_train) #모델 학습
  nb_pred = nb_classifier.predict(X_test) #X_test에 대한 예측값
  ```

###  24) 뉴스 분류 - Naive Bayes : 모델 평가 및 모델 성능 저장

```python
  print(Confusion_matrix(y_test, nb_pred) #Confusion matrix
  print(classification_report(y_test, nb_pred) #Classification report

  fm = round(f1_score(y_test, nb_pred, average = 'weighted'), 2) #실제 값과 예측 값을 비교해서 average = weighted를 적용하여 클래스 별 가중치를 적용하고 f_measure를 계산한다. round 함수로 소수점 2번째 자리까지 반올림한다.
  ac = round(accuracy_score(y_test, nb_pred, normalize = True), 2) #실제 값과 예측 값을 비교해서 Normalize = True로 정확도를 출력한다.
  df_per.loc[len(df_per)] = ['naive Bayes', fm, ac] #loc 함수를 사용해서 데이터 프레임에 인덱스를 지정해준다.
  df_per
#	Classifier	F-Measure	Accuracy
0	Naive bayes	0.56		0.55
```

###  25) 뉴스 분류 - Decision Tree : 모델 학습

```python
  from sklearn.tree import DecisionTreeClassifier #라이브러리 로드
  dt_classifier = DecisionTreeClassifier().fit(X_train, y_train) #모델 학습
  dt_pred = dt_classifier.predict(X_test) #X_test에 대한 예측값
```

###  26) 뉴스 분류 - Decision Tree : 모델 평가 및 모델 성능 저장

```python
  print(Confusion_matrix(y_test, dt_pred) #Confusion matrix
  print(classification_report(y_test, dt_pred) #Classification report

  fm = round(f1_score(y_test, dt_pred, average = 'weighted'), 2)
  ac = round(accuracy_score(y_test, dt_pred, normalize = True), 2)
  df_per.loc[len(df_per)] = ['Decision Tree', fm, ac]
  df_per
#	Classifier	F-Measure	Accuracy
0	Naive bayes	0.56		0.55
1	Decision Tree	0.39		0.39
```

###  27) 뉴스 분류 - Random forest : 모델 학습

```python
  from sklearn.ensemble import RandomForestClassifier #라이브러리 로드

  rf_classifier = RandomForestClassifier(n_estimators = 100)
  rf_classifier.fit(X_train, y_train) #모델 학습
  rf_pred = rf_classifier.predict(X_test) #X_test에 대한 예측값
```

###  28) 뉴스 분류 - Random forest : 모델 평가 및 모델 성능 저장

```python
  print(Confusion_matrix(y_test, rf_pred) #Confusion matrix
  print(classification_report(y_test, rf_pred) #Classification report

  fm = round(f1_score(y_test, rf_pred, average='weighted'), 2)
  ac = round(accuracy_score(y_test, rf_pred, normalize = True), 2)
  df_per.loc[len(df_per)] = ['Random Forest', fm, ac]
  df_per
#	Classifier	F-Measure	Accuracy
0	Naive bayes	0.56		0.55
1	Decision Tree	0.39		0.39
2	Random Forest	0.53		0.53
```

###  29) 뉴스 분류 - SVM : 모델 학습

```python
  from sklearn. svm import LinearSVC #라이브러리 로드

  SVM_classifier = LinearSVC().fit(X_train, y_train) #모델 학습
  SVM_pred = svm_classifier.predict(X_test) #X_test에 대한 예측값
```

###  30) 뉴스 분류 - SVM : 모델 평가 및 모델 성능 저장

```python
  print(Confusion_matrix(y_test, svm_pred) #Confusion matrix
  print(classification_report(y_test, svm_pred) #Classification report

  fm = round(f1_score(y_test, svm_pred, average='weighted'), 2)
  ac = round(accuracy_score(y_test, svm_pred, normalize = True), 2)
  df_per.loc[len(df_per)] = ['Support Vector Machine', fm, ac]
  df_per
#	Classifier		F-Measure	Accuracy
0	Naive bayes		0.56		0.55
1	Decision Tree		0.39		0.39
2	Random Forest		0.53		0.53
3	Support Vector Machine	0.57		0.56
```

###  31) 뉴스 분류 : 최종 성능 비교
   - 시각화를 위해 분류기 명을 set_index 함수를 사용해 index로 설정
   - kind = 'bar' : 막대 그래프
   - title = 'performance' : 그래프 제목
   - figsize : 그래프 크기 지정
   - legend : 데이터 설명
   - fontsize : 글씨 크기
   
   ```python
  df_per_1 = df_per.set_index('Classifier') #분류기 명을 set_index 함수를 사용해 index로 설정
  df_per_1

  ax = df_per_1[['F-Measure', 'Accuracy']].plot(kind='bar', title='performance', figsize=(10, 7), legend=True, fontsize=12)
  ax.set_xlabel('Classifier', fontsize=12) #그래프의 x축을 분류기 명으로 지정
  plt.show()
#Naive bayes와 SVM이 우수한 성능을 보였다.
```
