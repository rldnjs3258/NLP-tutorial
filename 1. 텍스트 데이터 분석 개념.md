# 1. 텍스트 데이터 분석 개념 (1)
## (1) 텍스트 분석이란?
  - 텍스트(비정형 데이터)로 부터 정보를 추출해 내는 작업이다.
  - 자연어 처리(NLP, Natural Language Processing)에 기반한다.
  
  <br>
  <hr>
  <br>

## (2) 텍스트 전처리 과정
  - 텍스트 분석을 위해서, 분석을 편하게 하기 위한 전처리 과정이 필요하다.
###  1) 텍스트 정규화(Text Normalization)
   - 텍스트의 형태를 일관되게 변형하는 작업이다.
   - 토큰화(Tokenization) : 텍스트를 의미 단위(토큰)로 분할하는 작업
   - 어간 추출(Stemming) : 어간추출은 의미 단위(토큰) 중 핵심을 추출하는 작업이다.

###  2) 형태소 분석(POS-Tagging)
   - 앞에서 토큰으로 잘라낸 단어들의 형태소를 파악하는 작업이다. (품사 정보 추출)

  
  <br>
  <hr>
  <br>


## (3) 텍스트 분석의 종류
###  1) 정보 추출(Information Retrieval)
   - 비정형 데이터인 문서 안에서 우리가 필요로 하는 정형 데이터 형태의 정보를 추출해내는 작업이다.

###  2) 문서 분류(Text Classification)
   - 문서들을 특정한 기준에 따라서 분류한다.
   ex) 뉴스를 분류할 때, 이 뉴스가 정치 카테고리에 속하는지 혹은 경제 카테고리에 속하는지 등 카테고리를 분류한다.

###  3) 감성 분석(Sentiment Analysis)
   - 문서에 내포되어 있는 감정과 의견을 추출해내는 작업이다.

  
  <br>
  <hr>
  <br>


## (4) 토큰화란?
  - 텍스트를 의미 단위로 분할하는 작업이다.
  - 텍스트를 잘게 자른 하나하나의 결과물을 토큰이라고 한다.

  
  <br>
  <hr>
  <br>


## (5) Token과 Type
  - Token은 텍스트 내부의 원소이고, Type은 단어요소, 어간이다.
  - Token : 공백을 기준으로 단순 분할한다.
  
    Type : 추출된 토큰들 중 중복을 제외하고 어간이 같은 단어(have와 had 등)를 제외한 것이 Type이다.

  
  <br>
  <hr>
  <br>


## (6) 토큰화 : 언어별 토큰화 방법
  - 영어 : 단순 공백 기준 분할
  - 독일어 : 복합 명사에 대한 분리 방법이 요구됨
  - 중국어 : 공백이 없으며 여러 문자로 한 단어가 이루어짐. 평균 2.4개의 문자들로 한 개의 단어가 구성됨.
  - 한국어 : 단순 공백 기준이 아닌 품사 기준(명사, 조사 등)으로 토큰
   ex) 한국어를 토큰화하는 예시 문장입니다.
   
       -> 한국어(명사), 를(조사), 토큰화, 하다, 예시, 문장, 입, 니다

  
  <br>
  <hr>
  <br>


## (7) 어간 추출
  - 어간 추출은 Lemmatization과 Stemming의 과정을 거친다.
###  1) Lemmatization
   - 문장 속에서 다양한 형태로 변형된 단어의 표제어를 찾는 작업이다.
   - 표제어 : 단어의 의미를 사전에서 찾을 때 사용하는 기본형.
   - 이러한 과정을 통해 flies가 '날다'(동사)인지 '파리'(명사)인지 파악한다.

###  2) Stemming
   - 형태가 변형된 단어에 대해 접사 등을 제거하고 어간을 분리해 내는 작업이다.
   
   - fly, flying, flies -> fly
   - stemming, stemmed, stemmer -> stem

  
  <br>
  <hr>
  <br>


## (8) 어간 추출 : 포터 알고리즘
  - 포터 알고리즘은 영어의 어간을 추출하는 대표적인 알고리즘이다.

###  1) Step 1a : 명사의 복수형을 단수형으로 고치는 규칙 적용
   - ss로 끝나는 단어를 sses로 고치면 단수형을 복수형으로 만든다. 이를 반대로 적용해서 복수형을 단수형으로 고친다.
   
     ex) caresses -> caress
   - i로 끝나는 단어를 ies로 고치면 단수형을 복수형으로 만든다. 이를 반대로 적용해서 복수형을 단수형으로 고친다.
   
     ex) ponies -> poni
   - ss로 끝나는 단어 중 추상 명사는 단수와 복수의 구분이 없다. 변화가 없다.
   
     ex) caress -> caress
   - s를 붙여서 단수형을 복수형으로 만든다. 이를 반대로 적용해서 복수형을 단수형으로 고친다.
   
     ex) cats -> cat

###  2) Step 1b : 단어의 품사가 바뀌면서 단어가 변형되는 경우 기본형으로 바꿈
   - eed로 끝나는 단어를 ee로 고친다.
   
     ex) agreed -> agree
   - ed로 끝나는 단어를 ed를 삭제한다.
   
     ex1) plastered -> plaster
     
     ex2) bled -> bled (조심!)
   - ing로 끝나는 단어를 ing을 삭제한다.
   
     ex1) motoring -> motor
     
     ex2) sing -> sing (조심!)

###  3) Step 1b1
   - at이 들어가게 변형된 단어를 ate로 고친다.
   
     ex) conflat(ed) -> conflate
   - bl이 들어가게 변형된 단어를 ble로 고친다.
   
     ex) troubl(ing) -> trouble
   - iz가 들어가게 변형된 단어를 ize로 고친다.
   
     ex) siz(ed) -> size

###  4) Step 2 : 단어의 어미에 변형이 일어난 경우 기본형으로 바꿈
   - ational로 끝나는 단어를 ate로 고친다.
   
     ex) relational -> relate
   - tional로 끝나는 단어를 tion으로 고친다.
   
     ex) conditional -> condition

###  5) Step 3, 4, 5a, 5b 과정 설명 생략

  
  <br>
  <hr>
  <br>


## (9) 형태소 분석이란?
  - 토큰의 품사 정보를 추출하는 작업
  - 토큰화를 한 다음 품사를 구분하는 작업이다!
  
  ex) Jane(NNP), play(VB), well(RB), her(PRP), friends(NNS)

  
  <br>
  <hr>
  <br>


## (10) 형태소 분석 : Open class vs Closed class
###  1) Open class
   - 상대적으로 새로운 단어의 등장이 많은 품사들이다.
   - 영어의 Open class는 Noun, Verbs, Adjectives, Adverbs 이다.
   
     명사 : Sejong University, Samsung, cat, table, ...
     
     동사 : listen, see, enter, ...
     
     형용사 : good, better, worse, ...
     
     부사 : slowly, hardly

###  2) Closed class
   - 상대적으로 새로운 단어의 등장이 많지 않은, 고정되어 있는 형태소들이다.
   - 주로 문장 내에서 문법적인 역할을 단어들이다.
   
     전치사 : of, in, at, ...
     
     조동사 : may, have, can, will, must, ...
     
     대명사 : I, you, she, mine, their, ...

  
  <br>
  <hr>
  <br>


## (11) 형태소 분석기
  - 영어 : Stanford POS-Tagger, NLTK POS-Tagger
  - 한국어 : Hannanum, KKma, Komoran, Twitter

  
  <br>
  <hr>
  <br>


## (12) 정보 추출(Information Retrieval, IR)
  - 앞에서 텍스트 분석을 위한 전처리의 종류를 살펴봤다. 이제 정보 추출에 대해서 알아보자.
  - 정보 추출은 구조화되어 있지 않은 문서 군집 안에서 필요한 정보를 포함하고 있는 문서를 찾아내는 작업이다. (문서는 테이블 형태로 표현될 수 있는 데이터를 뜻한다)
  
  ex) 인터넷 정보 검색

  
  <br>
  <hr>
  <br>


## (13) 정보 추출 - 단어 포함 여부와 빈도 : 6개의 동화 데이터를 예시로 정보 추출하기
  - 문제 : Cricket, Princess라고 하는 단어를 포함하면서 Pinocchio라고 하는 단어를 포함하고 있지 않은 동화는 어떤 것일까?
###   1) 문서 단어 행렬 만들기(Document term matrix)
    - 각 단어가 각 문서에 등장하는지의 유무를 행렬로 표현한다.
    - 해당 단어가 문서에 등장하면 1, 해당 단어가 문서에 등장하지 않으면 0이다.
    - 문서와 단어의 수가 증가하면 sparse matrix가 생성된다. (sparse matrix는 행렬의 원소에 비교적 0이 많은 행렬이다)
    - 각 단어는 벡터로 표현 가능하다.
    
     ex) Cricket과 Princess는 포함하되 Phinocchion는 포함하지 않는다는 조건의 벡터 연산 가능
     
         => Cricket AND Princess AND NOT Pinocchio
         
         => 110100 AND 110111 AND NOT 010000 = 100100
         
         => 조건에 만족하는 Alice와 Gulliver 찾을 수 있음!
         
         
| word/Document |	Alice |		Pinocchio |	Tailor |		Gulliver |	Little Prince |	Match Girl |
|---|:---:|---:|---:|---:|---:|---:|
| Fairy |		1 |		1 |		0 |		0 |		0 |		1 |
| Cricket |		1 |		1 |		0 |		1 |		0 |		0 |
| Princess |	1 |		1 |		0 |		1 |		1 |		0 |
| Pinocchio |	0 |		1 |		0 |		0 |		0 |		0 |
| Gryphon |		1 |		0 |		0 |		0 |		0 |		0 |
| Rose |		1 |		0 |		1 |		1 |		1 |		1 |
| Little |		1 |		0 |		1 |		1 |		1 |		1 |


###   2) 각 단어가 각 문서에 몇 번 등장하는지 빈도를 보여주는 행렬 만들기
    - 각 문서를 단어의 등장 빈도를 나타내는 count vector로 표현할 수 있다.
    - Pinocchio = (73, 157, 227, 10, 0, 0, 0)

| word/Document |	Alice |		Pinocchio |	Tailor |		Gulliver |	Little Prince |	Match Girl |
|---|:---:|---:|---:|---:|---:|---:|
| Fairy |		157 |		73 |		0 |		0 |		0 |		1 |
| Cricket |		4 |		157 |		0 |		2 |		0 |		0 |
| Princess |	232 |		227 |		0 |		2 |		1 |		0 |
| Pinocchio |	0 |		10 |		0 |		0 |		0 |		0 |
| Gryphon |		57 |		0 |		0 |		0 |		0 |		0 |
| Rose |		2 |		0 |		3 |		8 |		5 |		8 |
| Little |		2 |		0 |		1 |		1 |		1 |		5 |

  
  <br>
  <hr>
  <br>


## (14) 정보 추출 - 단어 포함 여부와 빈도 : Bag of words 모델
  - 문서 내에 등장하는 단어들의 순서를 무시하고, 각 문서를 단어의 등장 빈도를 통해 벡터 행렬로 표현한다.
  - 서로 의미가 다른 문장도, 같은 단어와 같은 빈도를 갖고 있으면 같은 벡터 형태로 표현된다.
  - 문제점 : 두 문장이 다르다고 하더라도 같은 형태로 표현됨.
  - John is quicker than Mary.
  - Mary is quicker than John.

  
  <br>
  <hr>
  <br>


## (15) 정보 추출 - 단어의 중요도 : Term Frequency(TF)
###  1) Term Frequency란?
   - TF(t,d)는 특정 단어 t가 모든 문서인 d에 등장하는 빈도이다.
   - TF 값이 크면 이 단어가 모든 문서에서 많이 등장한다는 뜻이다.
   - TF는 문서를 구분하는 척도로 사용하기에 적절하지 못하다.

###  2) TF와 타당성
   - 특정 단어가 모든 문서에서 10번 등장하는(tf = 10) 단어가 1번 등장하는 (tf=1) 단어보다 더 많이 타당하다고 볼 수 없다.
   - tf 값이 클 수록 모든 문서에 많이 등장하는 단어이지만, 정보가 부족한 단어일 가능성이 높다.
   
     모든 문서에서 드물게 등장하는 단어가 '다른 문서와 구별하는 데 사용할 수 있는' 더 중요한 정보일 수 있다.
   - 문서의 관련도, 타당성 : Term Frequency와 비례하게 증가하지 않는다.
      
   ex1) good, increase, high, of, the, a 단어는 그 문서와 다른 문서를 구분하기 쉽지 않다.
   
   ex2) 모든 문서에서 드물게 등장하는 back-propagation과 같은 단어는 이 문서와 다른 문서를 차별화 하는 단어이다.

  
  <br>
  <hr>
  <br>


## (16) 정보 추출 - 단어의 중요도 : Inverse Document Frequency(IDF)
###  1) Inverse Document Frequency란?
   - 드물게 등장하는 단어에 가중치를 부여한다.
   - df(t)는 단어 t가 등장한 문서의 수이다. df(t)에 역수를 취해서 단어 t의 중요도인 idf(t)를 구한다.
   
     idf(t)는 t의 단어의 중요도를 나타내는 척도이다.

###  2) IDF 예시
   - the와 같은 정관사는 df(t)의 값이 높다. 
   
     모든 문서에서 자주 나오기 때문에 하나의 문서를 다른 문서와 구별하는데 큰 영향을 주지 못한다.
   - Barbossa는 캐리비안의 해적에 나오는 캐릭터인데, 이 단어는 다른 문서에 나오기 힘든 단어이다.
   
     Barbossa는 그 문서를 다른 문서와 구별하는 데 큰 영향을 준다.
| Term	|	df(t)		| idf(t) |
|---|:---:|---:|
| Barbossa |	1 |		6 |
| Zoo |		100 |		5 |
| Monday |		1,000 |		4 |
| Sky |		10,000 |		3 |
| through |		100,000 |		2 |
| the |		1,000,000 |	1 |

###  3) Term Frequecny*Inverse Document Frequency(TF*IDF) 식
   - W(t,d) = (1+logtf(t,d) * log(N/df(t)))
   
     N : 문서 군집에 속하는 문서의 개수
   - TF와 IDF를 곱하면 문서군 내에서 특정 단어가 얼마나 중요한지를 수치화 할 수 있다.
   - 특정 문서 내 특정 단어의 빈도(tf)가 높아질 수록 값이 커지고,
   
     문서군 내에서 단어가 희박하게 등장할수록(idf) 커진다.

###  4) 단어의 TF*IDF값으로 이루어진 행렬

| word/Document |	Alice |		Pinocchio |	Tailor |		Gulliver |	Little Prince |	Match Girl |
|---|:---:|---:|---:|---:|---:|---:|
| Fairy |		5.25 |		3.18 |		0.0 |		0.0 |		0.0 |		0.35 |
| Cricket |		1.21 |		6.10 |		0.0 |		1.0 |		0.0 |		0.0 |
| Princess |	8.59 |		2.54 |		0.0 |		1.51 |		0.25 |		0.0 |
| Pinocchio |	0.0 |		1.51 |		0.0 |		0.0 |		0.0 |		0.0 |
| Gryphon |		2.85 |		0.0 |		0.0 |		0.0 |		0.0 |		0.0 |
| Rose |		1.51 |		0.0 |		1.90 |		0.12 |		5.25 |		0.88 |
| Little |		1.37 |		0.0 |		0.11 |		4.15 |		0.25 |		1.95 |
