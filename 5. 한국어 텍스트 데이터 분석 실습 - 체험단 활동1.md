# 1. 한국어 텍스트 데이터 분석 : 체험단 활동
##  1) 필요 패키지 import
  
  ```python
  import pandas as pd #데이터 프레임을 사용하기 위한 패키지
  import numpy as np #수학적 연산을 하기 위한 패키지
  ```

<br>
<hr>
<br>

##  2) 고객 데이터 csv 파일 읽어 데이터 프레임에 저장
  ```python
  df = pd.read_csv('customer_data(filtered).csv', encoding = 'cp949') #csv 파일을 read_csv를 사용해 데이터 프레임에 저장한다.
  df.head()
  ```  
  
<br>
<hr>
<br>


##  3) 한국어 형태소 분석기를 사용한 특징값 추출 : 이해하기
   - 텍스트 데이터 : 고객이 기업의 제품 체험단에 참여하기 위해 입력한 글
   - KoNLPy : 한국어의 명사와 형용사를 추출하는 4가지 한국어 형태소 분석기(Hannanum, Komoran, Kkma, Twitter) 중 Twitter 사용
   - Tf * idf : 고객의 의지가 표현된 특징값 추출


<br>
<hr>
<br>


##  4) 한국어 형태소 분석기를 사용한 특징값 추출 : KoNLPy 설치
   - JAVA 설치 : JDK 설치 -> JAVA_HOME 경로 설정
   - JPype 설치 : https://www.lfd.uci.edu/~gohlke/pythonlibs/#jpype -> anaconda의 python 버전에 맞는 JPype 설치 
-> 다운 파일 ProgramData/Anaconda3 폴더에 위치시키기 -> Anaconda prompt를 켜서 ProgramData/Anaconda3 위치로 가기 
-> pip install JPype1-0.6.3-cp37-cp37m-win_amd64.whl
   - KoNLPy 설치 : JPype과 마찬가지로 Anaconda prompt를 켜서 ProgramData/Anaconda3 위치로 가기 -> pip install konlpy


<br>
<hr>
<br>


##  5) 한국어 형태소 분석기를 사용한 특징값 추출 : Twitter import 및 선언

```python
  from konlpy.tag import Twitter #Twitter import
  twitter = Twitter() #형태소 분석기 twitter 생성
```

<br>
<hr>
<br>


##  6) 한국어 형태소 분석기를 사용한 특징값 추출 : Twitter를 통해 명사 및 형용사를 추출하는 함수 tokenize 생성
   - text를 분석해 형태소를 tag하여 tagged에 저장한다.
   - 형태소 분석 결과 tagged에 저장된 형태소가 'noun' 또는 'adjective'일 때 리스트 stems에 입력한다.
   
  ```python
  def tokenize(text) : #tokenize 함수를 만들어 text를 인자로 받는다.
	stems = [] #stems를 만들고 초기화
	tagged = twitter.pos(text) #pos() 함수는 주어진 text를 토큰화하여 품사를 판별하고 태그를 붙여준다.
	for i in range(0, len(tagged)): #tagged된 값 만큼 for문을 사용한다.
		#tagged[i][0]는 token, tagged[i][1]은 품사이다.
		if(tagged[i][1] == 'Noun' or tagged[i][1] == 'Adjective'): #tagged된 품사가 noun 또는 adjective일 때 변수 stem에 저장한다.
			stems.append(tagged[i][0])
	return stems
  ```


<br>
<hr>
<br>


##  7) Review 데이터 문서 단어 행렬 구성 : TF * IDF
   - 한국어 형태소 분석기를 사용해 추출한 토큰을 바탕으로 문서 단어 행렬 구성
   - TF * IDF 값을 산출해서 어떤 단어가 체험단 신청글에서 중요한 역할을 하는지 확인한다.
   
  ```python
  from sklearn.feature_extraction.text import TfidVectorizer #TF * IDF 값을 구하기 위한 패키지를 불러온다.

  #행렬을 만들기 위해 데이터의 형식을 변환한다.
  #짤막짤막한 리뷰 전체를 길게 늘여 하나로 묶는 것이다.
  text_data_list = df['Review'].astype(str).tolist() #astype으로 문자열 형태 리스트로 변환한다.
  text_data_arr = np.arry([''].join(text) for text in text data_list #join으로 text를 붙이고, array로 배열로 변환한다.

  #TF * IDF 값으로 단어 행렬을 생성하는 모듈 vectorizer 생성
  vectorizer = TfidVectorizer(min_df = 2, tokenizer = tokenize, norm = 'l2') #min_df로 단어의 최소 등장 빈도를 2로 설정하고, 앞에서 생성한 tokenize 함수를 사용하여 명사와 형용사를 추출한다. normalize를 위해 l2, pearson 함수를 사용해 표준화한다.
  text_data = vectorizer.fit_transform(text_data_arr) #fit_transform으로 문서단어행렬을 구하고 이 행렬을 'text_data에 저장한다.

  #TF * IDF 값으로 구한 문서 단어 행렬을 데이터 프레임 df_tfidf에 입력해 출력
  #거의다 0으로 채워져 있지만 문제 없다! 7~8백만개의 셀 중 값이 있는 건 6만 개 정도이다.
  df_tfidf = pd.DataFrame(text_data.A, columns = vectorizer.get_feature_names())
  df_tfidf.head()
  ```


<br>
<hr>
<br>

  
##  8) 데이터 셋 재구성 및 빈도 확인 : 주소 데이터 시각화 Seaborn
  
  ```python
  #지역 분포 시각화를 위해 패키지 import
  %matplotlib inline
  import matplotlib.pyplot as plt #그래프를 그리기 위한 시각화 도구
  import seaborn as sns #세련된 시각화를 위한 라이브러리

  #한글 폰트 사용을 위해 폰트 설정
  from matplotlib. import font_manager, rc #폰트를 관리하는 font_manager와 rc 임포트
  font_name = font_manager.FontProperties(fname = 'c:/windows/Font/malgun.ttf').get_name() #맑은 고딕 폰트를 사용하기 위해 font_name에 가져와 입력
  rc('font', family = font_name) #rc 함수를 사용해 위에서 지정한 폰트를 사용하도록 설정

  #Seaborn의 factorplot 함수를 사용해 막대 그래프 그리기
  g = sns.factorplot('addr', data = df, kind = 'count', size = 5) #addr 열을 설정해 주소 정보를 사용하고, 빈도수를 count하며, 이미지 크기를 5로 설정한다.
  g.set_xticklabels(rotation = 90) #x축의 이름을 라벨링 할 때 90도로 돌아간 이름을 라벨링 한다. (별 의미 없음)
  g.set_xlabels() #x축의 이름을 라벨링 한다. 경기도와 서울특별시의 고객이 압도적으로 많이 분포한다.
  ```


<br>
<hr>
<br>


##  9) 데이터 셋 재구성 및 빈도 확인 : SNS 데이터 시각화 Seaborn
  
  ```python
  g = sns.factorplot('SNS', data = df, kind = 'count', size = 5) #SNS 열을 설정해 SNS 정보를 사용하고, 빈도수를 count하며, 이미지 크기를 5로 설정한다.
  g.set_xlabels() #x축의 이름을 라벨링 한다. facebook과 kakaostory가 높은 빈도를 차지한다.
  ```


<br>
<hr>
<br>


##  10) 데이터 셋 재구성 및 빈도 확인 : 점수 데이터 시각화 Seaborn
   - 고객의 체험단 활동의 질인 점수를 확인한다.
   - 3점의 빈도수가 압도적으로 높았다.
   - 우리는 이 시각화 결과를 통해 1, 2, 3, 4, 5의 invariance를 줄이기 위해 1, 2를 묶고 4, 5를 묶어 볼 수 있다!
   - 우리는 1, 2점을 bad, 3을 normal, 4, 5점을 good으로 카테고리 할 수 있다.
   
  ```python
  g = sns.factorplot('Score', data = df, kind = 'count', size = 5) #Score 열을 설정해 점수 정보를 사용하고, 빈도수를 count하며, 이미지 크기를 5로 설정한다.
  g.set_xlabels()
  ```


<br>
<hr>
<br>


##  11) 데이터 셋 재구성 및 빈도 확인 : 점수 데이터의 통합 1
  
  ```python
  df = df.dropna(subset=['Score']) #점수가 없는 데이터를 제거하기 위해 dropna를 사용한다.
  df.index = range(0, len(df)) #데이터에 인덱스를 다시 부여한다.
  df['Score2'] = '' #점수를 기준으로 데이터를 정제한 뒤 통합할 점수를 입력할 score2를 생성한다.
  ```


<br>
<hr>
<br>


##  12) 데이터 셋 재구성 및 빈도 확인 : 점수 데이터의 통합 2
   - for문을 사용하여 1, 2점은 bad, 3점은 normal, 4, 5점은 good으로 통합한다.
   
  ```python
  for i in range(0, len(df)) : #for문으로 df를 돌림!
	if(df['Score'][i] < 3): #score가 1, 2인 경우
		df['Score2'][i] = 'bad' #bad
	elif (df['Score'][i] > 3) : #score가 4, 5인 경우
		df['Score2'][i] = 'good' #good
	elif (df['Score'][i] == 3) : #score가 3인 경우
		df['Score2'][i] = 'normal' #normal
  df
  ```


<br>
<hr>
<br>


##  13) 데이터 셋 재구성 및 빈도 확인 : 점수 데이터 통합 후 점수 데이터 시각화
   - 여전히 normal의 빈도가 높지만, good과 bad와의 차이가 감소해 invariance가 줄어듦을 알 수 있다.

  ```python
  g = sns.factorplot('Score2', data = df, kind = 'count', size = 5) #Score2 컬럼 값으로 빈도수를 구해본다.
  g.set_xlabels()
  ```


<br>
<hr>
<br>


##  14) 데이터 셋 재구성 및 빈도 확인 : 통합 결과의 데이터셋 저장
   - 완성한 데이터 셋을 to_csv 함수를 사용해 csv 파일로 저장한다.
   
  ```python
  df.to_csv('customer_data(filtered)_generated.csv')
  ```


<br>
<hr>
<br>


##  15) 데이터 셋 준비
   - 위에서 저장한 데이터 셋을 다시 불러온다.
   
  ```python
  df = pd.read_csv('customer_data(filtered)_generated.csv', encoding='cp949') #csv 파일을 읽어와 df에 저장
  df.head()
  ```


<br>
<hr>
<br>


##  16) 문서 단어 행렬 준비 : 형식 변환
   - Review를 독립 변수, Score2를 종속변수로 이용할 것이다!
   
  ```python
  # 데이터의 형식 변환 : Review 컬럼과 Score2 컬럼의 데이터를 astype으로 str로 변환한 후 tolist를 통해 리스트 형식으로 변환한다.
  review_data = df['Review'].astype(str).tolist()
  review_label = df['Score2'].astype(str).tolist()
  ```


<br>
<hr>
<br>


##  17) 문서 단어 행렬 준비 : 데이터 셋 분할
   - 데이터 셋의 80%는 training set으로, 나머지 20%는 test set으로 구성한다.
   
  ```python
  trainset_size = int(round(len(review_data) * 0.80)) #training set의 크기 설정

  # array 형식으로 변환한다. train set은 80%!
  x_train = np.array([''.join(data) for data in review_data[0:trainset_size]]) #review_data는 독립 변수
  y_train = np.array([data for data in review_label[0:trainset_size]]) #review_label은 종속 변수

  # array 형식으로 변환한다. test set은 나머지 20%!
  x_test = np.array([''.join(data) for data in review_data[trainset_size+1 : len(review_data)]]) #review_data는 독립 변수
  y_test = np.array([data for data in review_label[trainset_size + 1 : len(review_label)]]) #review_label은 종속 변수
  ```



<br>
<hr>
<br>


##  18) 문서 단어 행렬 생성 : TF * IDF
   - 독립 변수의 특징값을 추출하는 작업을 한다. 즉, 문서 단어 행렬을 만들어 본다.

  ```python
  X_train = vectorizer.fit_transform(x_train) #x_train을 기반으로 문서 단어 행렬을 구성한다.
  x_test = vectorizer.transform(x_test) #x_test을 기반으로 문서 단어 행렬을 구성한다.
  ```


<br>
<hr>
<br>


##  19) 성능 평가 준비 : 성능 평가 결과 입력 데이터 프레임 생성
   - 성능 평가 결과를 입력할 데이터 프레임을 생성한다. 컬럼은 classifier, f-measure, accuracy이다.
   
  ```python
  df_per = pd.DataFrame(columns = ['Classifier', 'F-Measure', 'Accuracy'])
  df_per
  ```


<br>
<hr>
<br>


##  20) 성능 평가 준비 : 성능 평가 패키지 import
  
  ```python
  from sklearn.metrics import confusion_matrix #분류 결과 건수 측정
  from sklearn.metrics import classification_report #recall, precision, f-measure 측정
  from sklearn.metrics import f1_score #f-measure 값 측정
  from sklearn.metrics import accuracy #accuracy 측정
  ```
